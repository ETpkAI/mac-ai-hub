---
sidebar_position: 1
sidebar_label: "GGUF 基础"
---

import RamCalc from '@site/src/components/RamCalc';

# 理解 GGUF 与内存

**GGUF**（GPT-Generated Unified Format）是本地运行大语言模型的标准格式。本指南解释其工作原理，并帮助您为 Mac 选择合适的模型。

## 什么是 GGUF？

GGUF 是由 [llama.cpp](https://github.com/ggml-org/llama.cpp) 项目设计的二进制格式。它取代了旧的 GGML 格式，带来了关键改进：

| 特性 | 优势 |
|------|------|
| **内存映射** | 通过 `mmap()` 实现模型即时加载 |
| **自包含** | 所有元数据存储在一个文件中 |
| **可扩展** | 轻松支持新架构 |
| **跨平台** | 适用于 Mac、Windows、Linux |

:::tip 对 Mac 的意义
内存映射意味着 macOS 只加载需要的页面，减少启动时间和内存压力。您的 7B 模型不需要一次性加载 5GB！
:::

## 理解量化

**量化**通过降低数值精度来减小模型大小。可以把它想象成图像的 JPEG 压缩——用一些质量换取更小的文件。

### 原始 vs 量化

| 格式 | 精度 | 模型大小 (7B) | 质量 |
|------|------|---------------|------|
| FP32 | 32位浮点 | ~28GB | 完美 |
| FP16 | 16位浮点 | ~14GB | 接近完美 |
| Q8_0 | 8位整数 | ~7GB | 优秀 |
| Q5_K_M | 5位混合 | ~5GB | 非常好 |
| **Q4_K_M** | 4位混合 | ~4GB | **良好（最佳平衡点）** |
| Q4_0 | 4位整数 | ~3.5GB | 可接受 |
| Q2_K | 2位 | ~2.5GB | 降级 |

### 量化中的 "K"

以 `_K` 结尾的量化（如 Q4_K_M）使用 **k-quant**，对不同层应用不同精度：
- **关键层**（注意力）：更高精度
- **非关键层**：更低精度

这在最大化压缩的同时保持了模型质量。

:::warning 避免 Q2_K 和 Q3_K
这些激进的量化会明显降低输出质量。坚持使用 Q4_K_M 或更高以获得良好结果。
:::

## 内存计算器

使用此计算器为您的 Mac 找到合适的模型：

<RamCalc />

## 内存计算说明

### 公式

```
所需内存 = 模型大小 × 1.2 + 上下文内存 + 系统保留
```

其中：
- **模型大小**：GGUF 文件大小
- **1.2 倍数**：工作内存开销
- **上下文内存**：8K 上下文约 2GB
- **系统保留**：macOS 需要 2-6GB

### 实际示例

| Mac | 总内存 | 系统占用 | 可用 | 最大模型 |
|-----|--------|----------|------|----------|
| M1 Air | 8GB | ~2GB | ~6GB | 7B Q4_K_M |
| M2 Pro | 16GB | ~4GB | ~12GB | 13B Q4_K_M |
| M3 Max | 64GB | ~6GB | ~58GB | 70B Q4_K_M |
| M2 Ultra | 192GB | ~10GB | ~180GB | 120B+ |

## 选择正确的量化

### 通用场景：Q4_K_M（推荐）
- 大小与质量的最佳平衡
- 适用于聊天、写作、编程
- 困惑度损失极小（约 0.5%）

### 编程/逻辑：Q5_K_M 或 Q8_0
- 更高精度保持推理能力
- 技术任务值得额外内存消耗

### 最大上下文：Q4_0
- 最小的实用大小
- 为更长对话释放内存

## 下载来源

:::tip 安全下载来源
- **HuggingFace** - [huggingface.co](https://huggingface.co)（官方模型）
- **TheBloke** - 社区量化（非常可靠）
- **Bartowski** - 高质量社区量化
:::

---

**下一步：** 查看我们的 [精选模型](/docs/models/top-models) 指南，获取针对 Mac 的具体推荐。
