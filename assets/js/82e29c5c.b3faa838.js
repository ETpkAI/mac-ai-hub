"use strict";(globalThis.webpackChunktemp_docusaurus=globalThis.webpackChunktemp_docusaurus||[]).push([[0],{282(e,i,n){n.r(i),n.d(i,{assets:()=>B,contentTitle:()=>v,default:()=>b,frontMatter:()=>_,metadata:()=>s,toc:()=>G});const s=JSON.parse('{"id":"models/gguf-basics","title":"Understanding GGUF & RAM","description":"GGUF (GPT-Generated Unified Format) is the standard format for running large language models locally. This guide explains how it works and helps you choose the right model for your Mac.","source":"@site/docs/models/gguf-basics.mdx","sourceDirName":"models","slug":"/models/gguf-basics","permalink":"/mac-ai-hub/docs/models/gguf-basics","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"sidebar_label":"GGUF Basics"},"sidebar":"tutorialSidebar","previous":{"title":"Model Vault","permalink":"/mac-ai-hub/docs/category/model-vault"},"next":{"title":"Top Models","permalink":"/mac-ai-hub/docs/models/top-models"}}');var l=n(4848),r=n(8453),t=n(6540);const d="calculator_jKJ9",a="header_dYhB",o="selector_BJcc",c="select_N3ca",h="results_KYbH",x="resultCard_Mm2J",m="resultLabel_Hq83",j="resultValue_ZcB1",u="examples_zMsK",g="warning_sSaN",p="note_ROkE",M={8:{maxModelSize:"7B",quantization:"Q4_K_M",examples:["Llama 3.1 8B","Mistral 7B","Gemma 2B"],warning:"Close other apps while running models for best performance."},16:{maxModelSize:"13B",quantization:"Q4_K_M",examples:["Llama 3.1 8B","Mistral 7B","Gemma 2 9B","Qwen 2.5 14B"]},24:{maxModelSize:"20B",quantization:"Q4_K_M or Q5_K_M",examples:["Mixtral 8x7B (MoE)","Qwen 2.5 14B","CodeLlama 20B"]},32:{maxModelSize:"30B",quantization:"Q4_K_M or Q5_K_M",examples:["Qwen 2.5 32B","DeepSeek Coder 33B","Mixtral 8x7B"]},64:{maxModelSize:"70B",quantization:"Q4_K_M",examples:["Llama 3.1 70B","Qwen 2.5 72B","Mixtral 8x22B"]},128:{maxModelSize:"120B+",quantization:"Q4_K_M or higher",examples:["Llama 3.1 70B (Q8)","Goliath 120B","Command R+ 104B"]},192:{maxModelSize:"180B+",quantization:"Q5_K_M or Q8_0",examples:["Any model at high quantization","Multiple models simultaneously"]}};function f(){const[e,i]=(0,t.useState)("16"),n=M[e];return(0,l.jsxs)("div",{className:d,children:[(0,l.jsxs)("div",{className:a,children:[(0,l.jsx)("h3",{children:"\ud83e\uddee RAM Calculator"}),(0,l.jsx)("p",{children:"Select your Mac's unified memory to see recommendations"})]}),(0,l.jsxs)("div",{className:o,children:[(0,l.jsx)("label",{htmlFor:"ram-select",children:"Your Mac's RAM:"}),(0,l.jsxs)("select",{id:"ram-select",value:e,onChange:e=>i(e.target.value),className:c,children:[(0,l.jsx)("option",{value:"8",children:"8GB (M1/M2/M3 Base)"}),(0,l.jsx)("option",{value:"16",children:"16GB (M1/M2/M3 Pro)"}),(0,l.jsx)("option",{value:"24",children:"24GB (M2/M3 Pro)"}),(0,l.jsx)("option",{value:"32",children:"32GB (M2/M3 Pro/Max)"}),(0,l.jsx)("option",{value:"64",children:"64GB (M2/M3 Max)"}),(0,l.jsx)("option",{value:"128",children:"128GB (M2/M3 Max/Ultra)"}),(0,l.jsx)("option",{value:"192",children:"192GB (M2 Ultra)"})]})]}),(0,l.jsxs)("div",{className:h,children:[(0,l.jsxs)("div",{className:x,children:[(0,l.jsx)("div",{className:m,children:"Maximum Model Size"}),(0,l.jsx)("div",{className:j,children:n.maxModelSize})]}),(0,l.jsxs)("div",{className:x,children:[(0,l.jsx)("div",{className:m,children:"Recommended Quantization"}),(0,l.jsx)("div",{className:j,children:n.quantization})]})]}),(0,l.jsxs)("div",{className:u,children:[(0,l.jsx)("strong",{children:"Recommended Models:"}),(0,l.jsx)("ul",{children:n.examples.map((e,i)=>(0,l.jsx)("li",{children:e},i))})]}),n.warning&&(0,l.jsxs)("div",{className:g,children:["\u26a0\ufe0f ",n.warning]}),(0,l.jsxs)("div",{className:p,children:[(0,l.jsx)("strong",{children:"Note:"})," Actual available memory is ~70-80% of total RAM after macOS system usage. These recommendations account for system overhead."]})]})}const _={sidebar_position:1,sidebar_label:"GGUF Basics"},v="Understanding GGUF & RAM",B={},G=[{value:"What is GGUF?",id:"what-is-gguf",level:2},{value:"Understanding Quantization",id:"understanding-quantization",level:2},{value:"Original vs Quantized",id:"original-vs-quantized",level:3},{value:"The &quot;K&quot; in Quantization",id:"the-k-in-quantization",level:3},{value:"RAM Calculator",id:"ram-calculator",level:2},{value:"Memory Math Explained",id:"memory-math-explained",level:2},{value:"The Formula",id:"the-formula",level:3},{value:"Real-World Examples",id:"real-world-examples",level:3},{value:"Choosing the Right Quantization",id:"choosing-the-right-quantization",level:2},{value:"For General Use: Q4_K_M (Recommended)",id:"for-general-use-q4_k_m-recommended",level:3},{value:"For Coding/Logic: Q5_K_M or Q8_0",id:"for-codinglogic-q5_k_m-or-q8_0",level:3},{value:"For Maximum Context: Q4_0",id:"for-maximum-context-q4_0",level:3},{value:"Download Sources",id:"download-sources",level:2}];function y(e){const i={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(i.header,{children:(0,l.jsx)(i.h1,{id:"understanding-gguf--ram",children:"Understanding GGUF & RAM"})}),"\n",(0,l.jsxs)(i.p,{children:[(0,l.jsx)(i.strong,{children:"GGUF"})," (GPT-Generated Unified Format) is the standard format for running large language models locally. This guide explains how it works and helps you choose the right model for your Mac."]}),"\n",(0,l.jsx)(i.h2,{id:"what-is-gguf",children:"What is GGUF?"}),"\n",(0,l.jsxs)(i.p,{children:["GGUF is a binary format designed by the ",(0,l.jsx)(i.a,{href:"https://github.com/ggml-org/llama.cpp",children:"llama.cpp"})," project. It replaced the older GGML format with key improvements:"]}),"\n",(0,l.jsxs)(i.table,{children:[(0,l.jsx)(i.thead,{children:(0,l.jsxs)(i.tr,{children:[(0,l.jsx)(i.th,{children:"Feature"}),(0,l.jsx)(i.th,{children:"Benefit"})]})}),(0,l.jsxs)(i.tbody,{children:[(0,l.jsxs)(i.tr,{children:[(0,l.jsx)(i.td,{children:(0,l.jsx)(i.strong,{children:"Memory Mapping"})}),(0,l.jsxs)(i.td,{children:["Models load instantly via ",(0,l.jsx)(i.code,{children:"mmap()"})]})]}),(0,l.jsxs)(i.tr,{children:[(0,l.jsx)(i.td,{children:(0,l.jsx)(i.strong,{children:"Self-Contained"})}),(0,l.jsx)(i.td,{children:"All metadata stored in one file"})]}),(0,l.jsxs)(i.tr,{children:[(0,l.jsx)(i.td,{children:(0,l.jsx)(i.strong,{children:"Extensible"})}),(0,l.jsx)(i.td,{children:"Supports new architectures easily"})]}),(0,l.jsxs)(i.tr,{children:[(0,l.jsx)(i.td,{children:(0,l.jsx)(i.strong,{children:"Cross-Platform"})}),(0,l.jsx)(i.td,{children:"Works on Mac, Windows, Linux"})]})]})]}),"\n",(0,l.jsx)(i.admonition,{title:"Why This Matters for Mac",type:"tip",children:(0,l.jsx)(i.p,{children:"Memory mapping means macOS loads only the pages it needs, reducing startup time and memory pressure. Your 7B model doesn't need 5GB loaded all at once!"})}),"\n",(0,l.jsx)(i.h2,{id:"understanding-quantization",children:"Understanding Quantization"}),"\n",(0,l.jsxs)(i.p,{children:[(0,l.jsx)(i.strong,{children:"Quantization"})," reduces model size by lowering numerical precision. Think of it like JPEG compression for images\u2014you trade some quality for much smaller files."]}),"\n",(0,l.jsx)(i.h3,{id:"original-vs-quantized",children:"Original vs Quantized"}),"\n",(0,l.jsxs)(i.table,{children:[(0,l.jsx)(i.thead,{children:(0,l.jsxs)(i.tr,{children:[(0,l.jsx)(i.th,{children:"Format"}),(0,l.jsx)(i.th,{children:"Precision"}),(0,l.jsx)(i.th,{children:"Model Size (7B)"}),(0,l.jsx)(i.th,{children:"Quality"})]})}),(0,l.jsxs)(i.tbody,{children:[(0,l.jsxs)(i.tr,{children:[(0,l.jsx)(i.td,{children:"FP32"}),(0,l.jsx)(i.td,{children:"32-bit float"}),(0,l.jsx)(i.td,{children:"~28GB"}),(0,l.jsx)(i.td,{children:"Perfect"})]}),(0,l.jsxs)(i.tr,{children:[(0,l.jsx)(i.td,{children:"FP16"}),(0,l.jsx)(i.td,{children:"16-bit float"}),(0,l.jsx)(i.td,{children:"~14GB"}),(0,l.jsx)(i.td,{children:"Near-perfect"})]}),(0,l.jsxs)(i.tr,{children:[(0,l.jsx)(i.td,{children:"Q8_0"}),(0,l.jsx)(i.td,{children:"8-bit int"}),(0,l.jsx)(i.td,{children:"~7GB"}),(0,l.jsx)(i.td,{children:"Excellent"})]}),(0,l.jsxs)(i.tr,{children:[(0,l.jsx)(i.td,{children:"Q5_K_M"}),(0,l.jsx)(i.td,{children:"5-bit mixed"}),(0,l.jsx)(i.td,{children:"~5GB"}),(0,l.jsx)(i.td,{children:"Very Good"})]}),(0,l.jsxs)(i.tr,{children:[(0,l.jsx)(i.td,{children:(0,l.jsx)(i.strong,{children:"Q4_K_M"})}),(0,l.jsx)(i.td,{children:"4-bit mixed"}),(0,l.jsx)(i.td,{children:"~4GB"}),(0,l.jsx)(i.td,{children:(0,l.jsx)(i.strong,{children:"Good (Sweet Spot)"})})]}),(0,l.jsxs)(i.tr,{children:[(0,l.jsx)(i.td,{children:"Q4_0"}),(0,l.jsx)(i.td,{children:"4-bit int"}),(0,l.jsx)(i.td,{children:"~3.5GB"}),(0,l.jsx)(i.td,{children:"Acceptable"})]}),(0,l.jsxs)(i.tr,{children:[(0,l.jsx)(i.td,{children:"Q2_K"}),(0,l.jsx)(i.td,{children:"2-bit"}),(0,l.jsx)(i.td,{children:"~2.5GB"}),(0,l.jsx)(i.td,{children:"Degraded"})]})]})]}),"\n",(0,l.jsx)(i.h3,{id:"the-k-in-quantization",children:'The "K" in Quantization'}),"\n",(0,l.jsxs)(i.p,{children:["Quantizations ending in ",(0,l.jsx)(i.code,{children:"_K"})," (like Q4_K_M) use ",(0,l.jsx)(i.strong,{children:"k-quant"}),", which applies different precision to different layers:"]}),"\n",(0,l.jsxs)(i.ul,{children:["\n",(0,l.jsxs)(i.li,{children:[(0,l.jsx)(i.strong,{children:"Critical layers"})," (attention): Higher precision"]}),"\n",(0,l.jsxs)(i.li,{children:[(0,l.jsx)(i.strong,{children:"Less critical layers"}),": Lower precision"]}),"\n"]}),"\n",(0,l.jsx)(i.p,{children:"This preserves model quality while maximizing compression."}),"\n",(0,l.jsx)(i.admonition,{title:"Avoid Q2_K and Q3_K",type:"warning",children:(0,l.jsx)(i.p,{children:"These aggressive quantizations noticeably degrade output quality. Stick to Q4_K_M or higher for good results."})}),"\n",(0,l.jsx)(i.h2,{id:"ram-calculator",children:"RAM Calculator"}),"\n",(0,l.jsx)(i.p,{children:"Use this calculator to find the right model for your Mac:"}),"\n",(0,l.jsx)(f,{}),"\n",(0,l.jsx)(i.h2,{id:"memory-math-explained",children:"Memory Math Explained"}),"\n",(0,l.jsx)(i.h3,{id:"the-formula",children:"The Formula"}),"\n",(0,l.jsx)(i.pre,{children:(0,l.jsx)(i.code,{children:"Required RAM = Model Size \xd7 1.2 + Context Memory + System Reserve\n"})}),"\n",(0,l.jsx)(i.p,{children:"Where:"}),"\n",(0,l.jsxs)(i.ul,{children:["\n",(0,l.jsxs)(i.li,{children:[(0,l.jsx)(i.strong,{children:"Model Size"}),": The GGUF file size"]}),"\n",(0,l.jsxs)(i.li,{children:[(0,l.jsx)(i.strong,{children:"1.2 multiplier"}),": Working memory overhead"]}),"\n",(0,l.jsxs)(i.li,{children:[(0,l.jsx)(i.strong,{children:"Context Memory"}),": ~2GB for 8K context"]}),"\n",(0,l.jsxs)(i.li,{children:[(0,l.jsx)(i.strong,{children:"System Reserve"}),": 2-6GB for macOS"]}),"\n"]}),"\n",(0,l.jsx)(i.h3,{id:"real-world-examples",children:"Real-World Examples"}),"\n",(0,l.jsxs)(i.table,{children:[(0,l.jsx)(i.thead,{children:(0,l.jsxs)(i.tr,{children:[(0,l.jsx)(i.th,{children:"Mac"}),(0,l.jsx)(i.th,{children:"Total RAM"}),(0,l.jsx)(i.th,{children:"System Use"}),(0,l.jsx)(i.th,{children:"Available"}),(0,l.jsx)(i.th,{children:"Max Model"})]})}),(0,l.jsxs)(i.tbody,{children:[(0,l.jsxs)(i.tr,{children:[(0,l.jsx)(i.td,{children:"M1 Air"}),(0,l.jsx)(i.td,{children:"8GB"}),(0,l.jsx)(i.td,{children:"~2GB"}),(0,l.jsx)(i.td,{children:"~6GB"}),(0,l.jsx)(i.td,{children:"7B Q4_K_M"})]}),(0,l.jsxs)(i.tr,{children:[(0,l.jsx)(i.td,{children:"M2 Pro"}),(0,l.jsx)(i.td,{children:"16GB"}),(0,l.jsx)(i.td,{children:"~4GB"}),(0,l.jsx)(i.td,{children:"~12GB"}),(0,l.jsx)(i.td,{children:"13B Q4_K_M"})]}),(0,l.jsxs)(i.tr,{children:[(0,l.jsx)(i.td,{children:"M3 Max"}),(0,l.jsx)(i.td,{children:"64GB"}),(0,l.jsx)(i.td,{children:"~6GB"}),(0,l.jsx)(i.td,{children:"~58GB"}),(0,l.jsx)(i.td,{children:"70B Q4_K_M"})]}),(0,l.jsxs)(i.tr,{children:[(0,l.jsx)(i.td,{children:"M2 Ultra"}),(0,l.jsx)(i.td,{children:"192GB"}),(0,l.jsx)(i.td,{children:"~10GB"}),(0,l.jsx)(i.td,{children:"~180GB"}),(0,l.jsx)(i.td,{children:"120B+"})]})]})]}),"\n",(0,l.jsx)(i.h2,{id:"choosing-the-right-quantization",children:"Choosing the Right Quantization"}),"\n",(0,l.jsx)(i.h3,{id:"for-general-use-q4_k_m-recommended",children:"For General Use: Q4_K_M (Recommended)"}),"\n",(0,l.jsxs)(i.ul,{children:["\n",(0,l.jsx)(i.li,{children:"Best balance of size vs quality"}),"\n",(0,l.jsx)(i.li,{children:"Works well for chat, writing, coding"}),"\n",(0,l.jsx)(i.li,{children:"Minimal perplexity loss (~0.5%)"}),"\n"]}),"\n",(0,l.jsx)(i.h3,{id:"for-codinglogic-q5_k_m-or-q8_0",children:"For Coding/Logic: Q5_K_M or Q8_0"}),"\n",(0,l.jsxs)(i.ul,{children:["\n",(0,l.jsx)(i.li,{children:"Higher precision preserves reasoning"}),"\n",(0,l.jsx)(i.li,{children:"Worth the extra memory for technical tasks"}),"\n"]}),"\n",(0,l.jsx)(i.h3,{id:"for-maximum-context-q4_0",children:"For Maximum Context: Q4_0"}),"\n",(0,l.jsxs)(i.ul,{children:["\n",(0,l.jsx)(i.li,{children:"Smallest practical size"}),"\n",(0,l.jsx)(i.li,{children:"Frees RAM for longer conversations"}),"\n"]}),"\n",(0,l.jsx)(i.h2,{id:"download-sources",children:"Download Sources"}),"\n",(0,l.jsx)(i.admonition,{title:"Safe Download Sources",type:"tip",children:(0,l.jsxs)(i.ul,{children:["\n",(0,l.jsxs)(i.li,{children:[(0,l.jsx)(i.strong,{children:"HuggingFace"})," - ",(0,l.jsx)(i.a,{href:"https://huggingface.co",children:"huggingface.co"})," (Official models)"]}),"\n",(0,l.jsxs)(i.li,{children:[(0,l.jsx)(i.strong,{children:"TheBloke"})," - Community quantizations (very reliable)"]}),"\n",(0,l.jsxs)(i.li,{children:[(0,l.jsx)(i.strong,{children:"Bartowski"})," - High-quality community quants"]}),"\n"]})}),"\n",(0,l.jsx)(i.hr,{}),"\n",(0,l.jsxs)(i.p,{children:[(0,l.jsx)(i.strong,{children:"Next:"})," Check out our ",(0,l.jsx)(i.a,{href:"/docs/models/top-models",children:"Top Models"})," guide for specific recommendations tailored to Mac."]})]})}function b(e={}){const{wrapper:i}={...(0,r.R)(),...e.components};return i?(0,l.jsx)(i,{...e,children:(0,l.jsx)(y,{...e})}):y(e)}},8453(e,i,n){n.d(i,{R:()=>t,x:()=>d});var s=n(6540);const l={},r=s.createContext(l);function t(e){const i=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function d(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:t(e.components),s.createElement(r.Provider,{value:i},e.children)}}}]);