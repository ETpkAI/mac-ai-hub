"use strict";(globalThis.webpackChunktemp_docusaurus=globalThis.webpackChunktemp_docusaurus||[]).push([[685],{5507(e,n,s){s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>t,default:()=>h,frontMatter:()=>d,metadata:()=>i,toc:()=>a});const i=JSON.parse('{"id":"models/top-models","title":"Essential Models for Mac","description":"A curated list of the best large language models optimized for Apple Silicon. All recommendations are in GGUF format for maximum compatibility.","source":"@site/docs/models/top-models.mdx","sourceDirName":"models","slug":"/models/top-models","permalink":"/mac-ai-hub/docs/models/top-models","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"sidebar_label":"Top Models"},"sidebar":"tutorialSidebar","previous":{"title":"GGUF Basics","permalink":"/mac-ai-hub/docs/models/gguf-basics"}}');var l=s(4848),r=s(8453);const d={sidebar_position:2,sidebar_label:"Top Models"},t="Essential Models for Mac",c={},a=[{value:"Quick Reference Table",id:"quick-reference-table",level:2},{value:"Detailed Recommendations",id:"detailed-recommendations",level:2},{value:"\ud83e\udd47 For 8GB Macs (M1/M2/M3 Base)",id:"-for-8gb-macs-m1m2m3-base",level:3},{value:"Llama 3.1 8B Instruct",id:"llama-31-8b-instruct",level:4},{value:"Mistral 7B Instruct v0.3",id:"mistral-7b-instruct-v03",level:4},{value:"\ud83e\udd48 For 16-18GB Macs (M2/M3 Pro)",id:"-for-16-18gb-macs-m2m3-pro",level:3},{value:"Gemma 2 9B Instruct",id:"gemma-2-9b-instruct",level:4},{value:"Qwen 2.5 14B Instruct",id:"qwen-25-14b-instruct",level:4},{value:"\ud83e\udd47 For 32-64GB Macs (M2/M3 Max)",id:"-for-32-64gb-macs-m2m3-max",level:3},{value:"Llama 3.1 70B Instruct",id:"llama-31-70b-instruct",level:4},{value:"Qwen 2.5 32B Instruct",id:"qwen-25-32b-instruct",level:4},{value:"\ud83d\udcbb For Coding Specifically",id:"-for-coding-specifically",level:3},{value:"How to Download",id:"how-to-download",level:2},{value:"Using Ollama (Easiest)",id:"using-ollama-easiest",level:3},{value:"Direct from HuggingFace",id:"direct-from-huggingface",level:3},{value:"Using <code>huggingface-cli</code>",id:"using-huggingface-cli",level:3},{value:"Performance Expectations",id:"performance-expectations",level:2},{value:"Tokens per Second by Hardware",id:"tokens-per-second-by-hardware",level:3}];function o(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.header,{children:(0,l.jsx)(n.h1,{id:"essential-models-for-mac",children:"Essential Models for Mac"})}),"\n",(0,l.jsx)(n.p,{children:"A curated list of the best large language models optimized for Apple Silicon. All recommendations are in GGUF format for maximum compatibility."}),"\n",(0,l.jsx)(n.h2,{id:"quick-reference-table",children:"Quick Reference Table"}),"\n",(0,l.jsxs)(n.table,{children:[(0,l.jsx)(n.thead,{children:(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.th,{children:"Model"}),(0,l.jsx)(n.th,{children:"Parameters"}),(0,l.jsx)(n.th,{children:"Min RAM"}),(0,l.jsx)(n.th,{children:"Recommended Quant"}),(0,l.jsx)(n.th,{children:"Best For"}),(0,l.jsx)(n.th,{children:"Download"})]})}),(0,l.jsxs)(n.tbody,{children:[(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:(0,l.jsx)(n.strong,{children:"Llama 3.1 8B"})}),(0,l.jsx)(n.td,{children:"8B"}),(0,l.jsx)(n.td,{children:"8GB"}),(0,l.jsx)(n.td,{children:"Q4_K_M"}),(0,l.jsx)(n.td,{children:"General chat, coding"}),(0,l.jsx)(n.td,{children:(0,l.jsx)(n.a,{href:"https://huggingface.co/meta-llama",children:"HuggingFace"})})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:(0,l.jsx)(n.strong,{children:"Mistral 7B"})}),(0,l.jsx)(n.td,{children:"7B"}),(0,l.jsx)(n.td,{children:"8GB"}),(0,l.jsx)(n.td,{children:"Q4_K_M"}),(0,l.jsx)(n.td,{children:"Fast responses, instruction-following"}),(0,l.jsx)(n.td,{children:(0,l.jsx)(n.a,{href:"https://huggingface.co/mistralai",children:"HuggingFace"})})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:(0,l.jsx)(n.strong,{children:"Gemma 2 9B"})}),(0,l.jsx)(n.td,{children:"9B"}),(0,l.jsx)(n.td,{children:"10GB"}),(0,l.jsx)(n.td,{children:"Q4_K_M"}),(0,l.jsx)(n.td,{children:"Balanced performance"}),(0,l.jsx)(n.td,{children:(0,l.jsx)(n.a,{href:"https://huggingface.co/google",children:"HuggingFace"})})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:(0,l.jsx)(n.strong,{children:"Llama 3.1 70B"})}),(0,l.jsx)(n.td,{children:"70B"}),(0,l.jsx)(n.td,{children:"48GB"}),(0,l.jsx)(n.td,{children:"Q4_K_M"}),(0,l.jsx)(n.td,{children:"Advanced reasoning"}),(0,l.jsx)(n.td,{children:(0,l.jsx)(n.a,{href:"https://huggingface.co/meta-llama",children:"HuggingFace"})})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:(0,l.jsx)(n.strong,{children:"Qwen 2.5 32B"})}),(0,l.jsx)(n.td,{children:"32B"}),(0,l.jsx)(n.td,{children:"24GB"}),(0,l.jsx)(n.td,{children:"Q4_K_M"}),(0,l.jsx)(n.td,{children:"Multilingual, coding"}),(0,l.jsx)(n.td,{children:(0,l.jsx)(n.a,{href:"https://huggingface.co/Qwen",children:"HuggingFace"})})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:(0,l.jsx)(n.strong,{children:"DeepSeek Coder"})}),(0,l.jsx)(n.td,{children:"33B"}),(0,l.jsx)(n.td,{children:"24GB"}),(0,l.jsx)(n.td,{children:"Q4_K_M"}),(0,l.jsx)(n.td,{children:"Code generation"}),(0,l.jsx)(n.td,{children:(0,l.jsx)(n.a,{href:"https://huggingface.co/deepseek-ai",children:"HuggingFace"})})]})]})]}),"\n",(0,l.jsx)(n.h2,{id:"detailed-recommendations",children:"Detailed Recommendations"}),"\n",(0,l.jsx)(n.h3,{id:"-for-8gb-macs-m1m2m3-base",children:"\ud83e\udd47 For 8GB Macs (M1/M2/M3 Base)"}),"\n",(0,l.jsx)(n.p,{children:"Your options are limited but still powerful:"}),"\n",(0,l.jsx)(n.h4,{id:"llama-31-8b-instruct",children:"Llama 3.1 8B Instruct"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"File: Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf\nSize: ~4.9GB\nSpeed: ~15-20 tokens/sec\n"})}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Pros:"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Excellent instruction following"}),"\n",(0,l.jsx)(n.li,{children:"Strong coding capabilities"}),"\n",(0,l.jsx)(n.li,{children:"Great at reasoning tasks"}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Cons:"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"8B limit means less knowledge depth"}),"\n"]}),"\n",(0,l.jsx)(n.admonition,{title:"Memory Tip",type:"tip",children:(0,l.jsx)(n.p,{children:"Close Safari and other apps before running. Every GB counts on 8GB machines!"})}),"\n",(0,l.jsx)(n.hr,{}),"\n",(0,l.jsx)(n.h4,{id:"mistral-7b-instruct-v03",children:"Mistral 7B Instruct v0.3"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"File: Mistral-7B-Instruct-v0.3-Q4_K_M.gguf\nSize: ~4.4GB\nSpeed: ~20-25 tokens/sec\n"})}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Pros:"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Fastest 7B model"}),"\n",(0,l.jsx)(n.li,{children:"Excellent at following instructions"}),"\n",(0,l.jsx)(n.li,{children:"Lower memory usage"}),"\n"]}),"\n",(0,l.jsx)(n.hr,{}),"\n",(0,l.jsx)(n.h3,{id:"-for-16-18gb-macs-m2m3-pro",children:"\ud83e\udd48 For 16-18GB Macs (M2/M3 Pro)"}),"\n",(0,l.jsx)(n.p,{children:"You can run medium-sized models comfortably:"}),"\n",(0,l.jsx)(n.h4,{id:"gemma-2-9b-instruct",children:"Gemma 2 9B Instruct"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"File: gemma-2-9b-it-Q5_K_M.gguf\nSize: ~6.5GB\nSpeed: ~12-18 tokens/sec\n"})}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Pros:"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Google's latest architecture"}),"\n",(0,l.jsx)(n.li,{children:"Excellent at nuanced responses"}),"\n",(0,l.jsx)(n.li,{children:"Strong multilingual support"}),"\n"]}),"\n",(0,l.jsx)(n.hr,{}),"\n",(0,l.jsx)(n.h4,{id:"qwen-25-14b-instruct",children:"Qwen 2.5 14B Instruct"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"File: Qwen2.5-14B-Instruct-Q4_K_M.gguf\nSize: ~8.5GB\nSpeed: ~10-15 tokens/sec\n"})}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Pros:"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Excellent for Chinese + English"}),"\n",(0,l.jsx)(n.li,{children:"Strong coding abilities"}),"\n",(0,l.jsx)(n.li,{children:"Great general knowledge"}),"\n"]}),"\n",(0,l.jsx)(n.hr,{}),"\n",(0,l.jsx)(n.h3,{id:"-for-32-64gb-macs-m2m3-max",children:"\ud83e\udd47 For 32-64GB Macs (M2/M3 Max)"}),"\n",(0,l.jsx)(n.p,{children:"Access to powerful large models:"}),"\n",(0,l.jsx)(n.h4,{id:"llama-31-70b-instruct",children:"Llama 3.1 70B Instruct"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"File: Meta-Llama-3.1-70B-Instruct-Q4_K_M.gguf\nSize: ~40GB\nSpeed: ~8-12 tokens/sec\n"})}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Pros:"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Near GPT-4 quality"}),"\n",(0,l.jsx)(n.li,{children:"Exceptional reasoning"}),"\n",(0,l.jsx)(n.li,{children:"Comprehensive knowledge"}),"\n"]}),"\n",(0,l.jsx)(n.admonition,{title:"Requires 48GB+ RAM",type:"warning",children:(0,l.jsx)(n.p,{children:"70B models need significant memory. Use Q4_K_M quantization and close all other apps."})}),"\n",(0,l.jsx)(n.hr,{}),"\n",(0,l.jsx)(n.h4,{id:"qwen-25-32b-instruct",children:"Qwen 2.5 32B Instruct"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"File: Qwen2.5-32B-Instruct-Q4_K_M.gguf\nSize: ~19GB\nSpeed: ~12-16 tokens/sec\n"})}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Pros:"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Best-in-class for 32B size"}),"\n",(0,l.jsx)(n.li,{children:"Excellent coding"}),"\n",(0,l.jsx)(n.li,{children:"Fast for its size"}),"\n"]}),"\n",(0,l.jsx)(n.hr,{}),"\n",(0,l.jsx)(n.h3,{id:"-for-coding-specifically",children:"\ud83d\udcbb For Coding Specifically"}),"\n",(0,l.jsxs)(n.table,{children:[(0,l.jsx)(n.thead,{children:(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.th,{children:"Model"}),(0,l.jsx)(n.th,{children:"Parameters"}),(0,l.jsx)(n.th,{children:"Specialty"})]})}),(0,l.jsxs)(n.tbody,{children:[(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:(0,l.jsx)(n.strong,{children:"DeepSeek Coder V2"})}),(0,l.jsx)(n.td,{children:"16B"}),(0,l.jsx)(n.td,{children:"General coding"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:(0,l.jsx)(n.strong,{children:"CodeLlama 34B"})}),(0,l.jsx)(n.td,{children:"34B"}),(0,l.jsx)(n.td,{children:"Python, JavaScript"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:(0,l.jsx)(n.strong,{children:"Qwen 2.5 Coder"})}),(0,l.jsx)(n.td,{children:"7B-32B"}),(0,l.jsx)(n.td,{children:"Multi-language"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:(0,l.jsx)(n.strong,{children:"StarCoder2"})}),(0,l.jsx)(n.td,{children:"15B"}),(0,l.jsx)(n.td,{children:"Multi-language"})]})]})]}),"\n",(0,l.jsx)(n.admonition,{title:"Coding Model Tips",type:"tip",children:(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Use Q5_K_M or Q8_0 for code\u2014precision matters for syntax"}),"\n",(0,l.jsx)(n.li,{children:"Increase context length for larger codebases"}),"\n",(0,l.jsx)(n.li,{children:"Consider models fine-tuned for your language"}),"\n"]})}),"\n",(0,l.jsx)(n.h2,{id:"how-to-download",children:"How to Download"}),"\n",(0,l.jsx)(n.h3,{id:"using-ollama-easiest",children:"Using Ollama (Easiest)"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"ollama pull llama3.1:8b\nollama pull mistral:7b\nollama pull gemma2:9b\n"})}),"\n",(0,l.jsx)(n.h3,{id:"direct-from-huggingface",children:"Direct from HuggingFace"}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:["Visit ",(0,l.jsx)(n.a,{href:"https://huggingface.co",children:"huggingface.co"})]}),"\n",(0,l.jsx)(n.li,{children:"Search for the model"}),"\n",(0,l.jsxs)(n.li,{children:["Go to ",(0,l.jsx)(n.strong,{children:"Files and versions"})]}),"\n",(0,l.jsxs)(n.li,{children:["Download the ",(0,l.jsx)(n.code,{children:".gguf"})," file matching your RAM"]}),"\n"]}),"\n",(0,l.jsxs)(n.h3,{id:"using-huggingface-cli",children:["Using ",(0,l.jsx)(n.code,{children:"huggingface-cli"})]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-bash",children:"pip install huggingface_hub\nhuggingface-cli download TheBloke/Llama-2-7B-GGUF llama-2-7b.Q4_K_M.gguf\n"})}),"\n",(0,l.jsx)(n.h2,{id:"performance-expectations",children:"Performance Expectations"}),"\n",(0,l.jsx)(n.h3,{id:"tokens-per-second-by-hardware",children:"Tokens per Second by Hardware"}),"\n",(0,l.jsxs)(n.table,{children:[(0,l.jsx)(n.thead,{children:(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.th,{children:"Hardware"}),(0,l.jsx)(n.th,{children:"7B Model"}),(0,l.jsx)(n.th,{children:"13B Model"}),(0,l.jsx)(n.th,{children:"70B Model"})]})}),(0,l.jsxs)(n.tbody,{children:[(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"M1 8GB"}),(0,l.jsx)(n.td,{children:"15-20 t/s"}),(0,l.jsx)(n.td,{children:"\u274c"}),(0,l.jsx)(n.td,{children:"\u274c"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"M2 Pro 16GB"}),(0,l.jsx)(n.td,{children:"20-25 t/s"}),(0,l.jsx)(n.td,{children:"12-18 t/s"}),(0,l.jsx)(n.td,{children:"\u274c"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"M3 Max 64GB"}),(0,l.jsx)(n.td,{children:"25-30 t/s"}),(0,l.jsx)(n.td,{children:"18-22 t/s"}),(0,l.jsx)(n.td,{children:"8-12 t/s"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"M2 Ultra 192GB"}),(0,l.jsx)(n.td,{children:"30+ t/s"}),(0,l.jsx)(n.td,{children:"22-28 t/s"}),(0,l.jsx)(n.td,{children:"15-20 t/s"})]})]})]}),"\n",(0,l.jsx)(n.hr,{}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Previous:"})," ",(0,l.jsx)(n.a,{href:"/docs/models/gguf-basics",children:"GGUF Basics"})," - Understanding quantization and memory requirements"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(o,{...e})}):o(e)}},8453(e,n,s){s.d(n,{R:()=>d,x:()=>t});var i=s(6540);const l={},r=i.createContext(l);function d(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:d(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);